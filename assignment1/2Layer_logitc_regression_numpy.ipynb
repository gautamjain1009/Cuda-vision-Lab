{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle as skshuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n",
    "\n",
    "* Data used : https://www.kaggle.com/oddrationale/mnist-in-csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(train_file_path, test_file_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    load mnist data\n",
    "    \n",
    "    inputs : train, test file paths\n",
    "    outputs : parsed train, test images and labels\n",
    "    \"\"\"\n",
    "    \n",
    "    training_data = pd.read_csv(train_file_path)\n",
    "    testing_data = pd.read_csv(test_file_path)\n",
    "    train_data = training_data.to_numpy()\n",
    "    \n",
    "    test_data = testing_data.to_numpy()\n",
    "    train_images = train_data[:,1:]\n",
    "    train_labels = train_data[:,0][:,np.newaxis]\n",
    "    test_images = test_data[:,1:]\n",
    "    test_labels = test_data[:,0][:,np.newaxis]\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(num_classes, num_inputs, batch_size):\n",
    "    \"\"\"\n",
    "    initialize weight and bias matrix\n",
    "    \n",
    "    inputs : number of classes, number of input neurons, batch size\n",
    "    outputs : randomly initialized weight and bias matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.random.randn(num_classes, num_inputs) / np.sqrt(num_classes*num_inputs) \n",
    "    b = np.random.randn(num_classes, batch_size) / np.sqrt(num_classes)  \n",
    "  \n",
    "    return w,b \n",
    "\n",
    "def MSE_loss(y,pred):\n",
    "    \"\"\"\n",
    "    Calculate Mean square error\n",
    "    \n",
    "    inputs: dataset labels and the predicated labels \n",
    "    outputs: Loss value\n",
    "    \n",
    "    \"\"\"\n",
    "    loss = np.sum(np.power((y-pred),2), axis=0)\n",
    "\n",
    "    return np.sum(loss) \n",
    "\n",
    "def sigmoid(output):\n",
    "    \"\"\"\n",
    "    sigmoid activation function\n",
    "    \n",
    "    input: ouput from a layer\n",
    "    ouput: outputs after sigmoid activation function\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-output))\n",
    "    \n",
    "    return s \n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    softmax activation function generally used in the last layer\n",
    "    \n",
    "    input: output after the last hidden layer\n",
    "    output: class probablities\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    scores = np.exp(x-np.max(x))\n",
    "    prob = (scores.T / np.sum(scores, axis=1)).T\n",
    "    return prob\n",
    "    \n",
    "\n",
    "def get_minibatches(x,y,batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create minibatches from the data and shuffle them \n",
    "    \n",
    "    inputs: images, labels, batch size, shuffle enabled\n",
    "    outputs: shuffled batches of images and labels according to batch size \n",
    "    \"\"\"\n",
    "    minibatches = []\n",
    "    x_shuff, y_shuff = np.copy(x), np.copy(y)\n",
    "    if shuffle:\n",
    "        x_shuff, y_shuff = skshuffle(x_shuff, y_shuff)\n",
    "    for i in range(0, x_shuff.shape[0], batch_size):\n",
    "        yield x_shuff[i:i + batch_size], y_shuff[i:i+batch_size] \n",
    "\n",
    "\n",
    "\n",
    "def feed_forward_network(w1,w2,b1,b2,input_data):\n",
    "    \"\"\"\n",
    "    Simple 2 layer feedforward neural network \n",
    "    \n",
    "    inputs: initialised weight, bias matrix and input data \n",
    "    ouputs: ouputs after 1 and 2 layer\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    Z1 = np.dot(w1,input_data) + b1\n",
    "\n",
    "    A1 = sigmoid(Z1)\n",
    "   \n",
    "    Z2 = np.dot(w2,A1) + b2 \n",
    "\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    return A1,A2, Z1\n",
    "\n",
    "# feed_forward_network(W1,W2,B1,B2,inp.T)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def training_loop(X,Y,num_batch,nm_iters,num_class,hid_layer_neurons,in_neurons):\n",
    "    \"\"\"\n",
    "    training loop : updating weight by stochastic gradient decent. \n",
    "    \n",
    "    inputs: input images and labels, batch size, number of iterations, number of classes, \n",
    "            number of hidden layer neurons, input size\n",
    "    outputs: loss after every epoch, learned weight and bias matrix \n",
    "    \n",
    "    \"\"\"\n",
    "    ### feed forward network -- 2 layers\n",
    "    W1, B1  = initialize(hid_layer_neurons,in_neurons,num_batch)\n",
    "    W2 ,B2 =  initialize(num_class,hid_layer_neurons,num_batch)\n",
    "    \n",
    "    loss_iter = []\n",
    "    for i in range(nm_iters):\n",
    "        minibatch = get_minibatches(X,Y, num_batch, shuffle = True)\n",
    "        \n",
    "        minibatch_loss = []\n",
    "        \n",
    "        for x_train_minibatch, y_train_minibatch in minibatch:    \n",
    "            \n",
    "            layer_1_out, layer_2_out, Z1 = feed_forward_network(W1,W2,B1,B2,x_train_minibatch.T)\n",
    "            loss = MSE_loss(y_train_minibatch.T,layer_2_out)\n",
    "           \n",
    "            ### computing gradients \n",
    "            error = layer_2_out - y_train_minibatch.T\n",
    "     \n",
    "            #last layer \n",
    "            dW2 = (1/num_batch) * np.dot(error, layer_1_out.T )\n",
    "            dB2 = (1/num_batch) * np.sum(error, axis = 1, keepdims = True)\n",
    "\n",
    "            #propagating gradients through first layer\n",
    "            dA1 = np.dot(W2.T, error)\n",
    "            dZ1 = dA1 * sigmoid(Z1) * (1- sigmoid(Z1))\n",
    "\n",
    "            #first layer \n",
    "            dW1 = (1/num_batch) * np.dot(dZ1 , x_train_minibatch)\n",
    "            dB1 = (1/num_batch) * np.sum(dZ1,  axis = 1,  keepdims = True)\n",
    "     \n",
    "            ### updating the weights and bias by stochastic gradient decent \n",
    "            \n",
    "            W1 = W1 - learning_rate*dW1\n",
    "            W2 = W2 - learning_rate*dW2\n",
    "            \n",
    "            B1 = B1 - learning_rate*dB1\n",
    "            B2 = B2 - learning_rate*dB2\n",
    "            \n",
    "            \n",
    "            minibatch_loss.append(loss)\n",
    "            \n",
    "        batch_loss = np.mean(minibatch_loss)\n",
    "        print(\"LOSS:\",batch_loss)\n",
    "        loss_iter.append(batch_loss)\n",
    "        \n",
    "        \n",
    "    return loss_iter, W1, W2, B1, B2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 18.087593530090725\n",
      "LOSS: 17.629092713342864\n",
      "LOSS: 17.429102628402877\n",
      "LOSS: 17.274236987253836\n",
      "LOSS: 17.129096067659944\n",
      "LOSS: 16.998258673792957\n",
      "LOSS: 16.871289429474803\n",
      "LOSS: 16.746093803070767\n",
      "LOSS: 16.624906993869686\n",
      "LOSS: 16.498825900101803\n",
      "LOSS: 16.389364249514184\n",
      "LOSS: 16.24573172305202\n",
      "LOSS: 16.130716064754512\n",
      "LOSS: 16.005968289572213\n",
      "LOSS: 15.87644598428758\n",
      "LOSS: 15.758094825175766\n",
      "LOSS: 15.643643019168254\n",
      "LOSS: 15.516794163344333\n",
      "LOSS: 15.36657454940402\n",
      "LOSS: 15.237812174722817\n",
      "LOSS: 15.113513724232408\n",
      "LOSS: 14.995541023301264\n",
      "LOSS: 14.868341907894811\n",
      "LOSS: 14.744793532228053\n",
      "LOSS: 14.623225881680609\n",
      "LOSS: 14.518463979370082\n",
      "LOSS: 14.38821402146997\n",
      "LOSS: 14.285821972243255\n",
      "LOSS: 14.1587336786303\n",
      "LOSS: 14.050140807325796\n",
      "LOSS: 13.93913479177156\n",
      "LOSS: 13.843492768775429\n",
      "LOSS: 13.720751292466714\n",
      "LOSS: 13.623738181623024\n",
      "LOSS: 13.537980513222264\n",
      "LOSS: 13.427875353871821\n",
      "LOSS: 13.342729946541157\n",
      "LOSS: 13.225055596007103\n",
      "LOSS: 13.141461044430182\n",
      "LOSS: 13.044806669579113\n",
      "LOSS: 12.961395998298586\n",
      "LOSS: 12.869659621389399\n",
      "LOSS: 12.780803396296665\n",
      "LOSS: 12.707413442609482\n",
      "LOSS: 12.620019442762963\n",
      "LOSS: 12.537309874912328\n",
      "LOSS: 12.474418190243837\n",
      "LOSS: 12.398622585459341\n",
      "LOSS: 12.312803258854279\n",
      "LOSS: 12.255597964238738\n",
      "LOSS: 12.178019607311722\n",
      "LOSS: 12.106164153851484\n",
      "LOSS: 12.054217025433354\n",
      "LOSS: 11.985799765301072\n",
      "LOSS: 11.940414488346724\n",
      "LOSS: 11.874812444471647\n",
      "LOSS: 11.797476906618016\n",
      "LOSS: 11.742092472492441\n",
      "LOSS: 11.672109616894472\n",
      "LOSS: 11.627840234870408\n",
      "LOSS: 11.590332370374133\n",
      "LOSS: 11.513198378812053\n",
      "LOSS: 11.472680409156082\n",
      "LOSS: 11.399654267594105\n",
      "LOSS: 11.361714183278876\n",
      "LOSS: 11.33744743654794\n",
      "LOSS: 11.269620540441492\n",
      "LOSS: 11.214488160561082\n",
      "LOSS: 11.191921062183178\n",
      "LOSS: 11.136147234847469\n",
      "LOSS: 11.10270695839461\n",
      "LOSS: 11.070992077065418\n",
      "LOSS: 11.004571002152215\n",
      "LOSS: 10.980584472246633\n",
      "LOSS: 10.957553778028043\n",
      "LOSS: 10.916827435763746\n",
      "LOSS: 10.878386141574097\n",
      "LOSS: 10.858361315187913\n",
      "LOSS: 10.804797518904536\n",
      "LOSS: 10.770812116154925\n",
      "LOSS: 10.74970224728037\n",
      "LOSS: 10.675406883007929\n",
      "LOSS: 10.679793588442713\n",
      "LOSS: 10.65781005926054\n",
      "LOSS: 10.596379505947343\n",
      "LOSS: 10.569691365031325\n",
      "LOSS: 10.568647861904303\n",
      "LOSS: 10.516494236155658\n",
      "LOSS: 10.507651721857902\n",
      "LOSS: 10.463447965886525\n",
      "LOSS: 10.437068292563733\n",
      "LOSS: 10.410640052705421\n",
      "LOSS: 10.419584641784592\n",
      "LOSS: 10.36426620666168\n",
      "LOSS: 10.35880133068402\n",
      "LOSS: 10.3325748458605\n",
      "LOSS: 10.300843116563096\n",
      "LOSS: 10.295249418864442\n",
      "LOSS: 10.275863164538169\n",
      "LOSS: 10.25270106456486\n",
      "LOSS: 10.206826664061404\n",
      "LOSS: 10.217198058676706\n",
      "LOSS: 10.17931128993778\n",
      "LOSS: 10.172249688469261\n",
      "LOSS: 10.170664897642387\n",
      "LOSS: 10.129880413356897\n",
      "LOSS: 10.125047497263566\n",
      "LOSS: 10.10762428279514\n",
      "LOSS: 10.08223570951648\n",
      "LOSS: 10.053471519846214\n",
      "LOSS: 10.043269159012995\n",
      "LOSS: 10.008689601675565\n",
      "LOSS: 10.014811449770722\n",
      "LOSS: 10.019439847725812\n",
      "LOSS: 9.974797976827666\n",
      "LOSS: 9.977426265966738\n",
      "LOSS: 9.97774735222454\n",
      "LOSS: 9.922369824858212\n",
      "LOSS: 9.940943352752427\n",
      "LOSS: 9.904145689525786\n",
      "LOSS: 9.919363142065407\n",
      "LOSS: 9.910349870428318\n",
      "LOSS: 9.879281005491958\n",
      "LOSS: 9.8884984794966\n",
      "LOSS: 9.877203654014684\n",
      "LOSS: 9.874345141302763\n",
      "LOSS: 9.814619047095166\n",
      "LOSS: 9.855079842010872\n",
      "LOSS: 9.845670251282444\n",
      "LOSS: 9.825627783581988\n",
      "LOSS: 9.752160165520376\n",
      "LOSS: 9.772152017698048\n",
      "LOSS: 9.791203886186317\n",
      "LOSS: 9.748020299561528\n",
      "LOSS: 9.795790187002705\n",
      "LOSS: 9.743123812903914\n",
      "LOSS: 9.75131891070404\n",
      "LOSS: 9.71145926551046\n",
      "LOSS: 9.738988285246224\n",
      "LOSS: 9.738745985321984\n",
      "LOSS: 9.69673589829425\n",
      "LOSS: 9.701505185841729\n",
      "LOSS: 9.67234347475393\n",
      "LOSS: 9.692079314501706\n",
      "LOSS: 9.647279218840483\n",
      "LOSS: 9.665508129016484\n",
      "LOSS: 9.660459265197764\n",
      "LOSS: 9.647562521151446\n",
      "LOSS: 9.642897027632127\n",
      "LOSS: 9.621661971261688\n",
      "LOSS: 9.61684793355021\n",
      "LOSS: 9.618008772533111\n",
      "LOSS: 9.59224555176018\n",
      "LOSS: 9.577215919061786\n",
      "LOSS: 9.561594653071632\n",
      "LOSS: 9.61126539613301\n",
      "LOSS: 9.572580882561816\n",
      "LOSS: 9.584486580944853\n",
      "LOSS: 9.591418281627579\n",
      "LOSS: 9.548032964319097\n",
      "LOSS: 9.538429434098722\n",
      "LOSS: 9.544438324542096\n",
      "LOSS: 9.520357247972772\n",
      "LOSS: 9.537313542386583\n",
      "LOSS: 9.570256591441266\n",
      "LOSS: 9.512980981516254\n",
      "LOSS: 9.526815577935496\n",
      "LOSS: 9.52092461786591\n",
      "LOSS: 9.534139357375059\n",
      "LOSS: 9.520999712757513\n",
      "LOSS: 9.494236336242764\n",
      "LOSS: 9.494348924623933\n",
      "LOSS: 9.498395804371265\n",
      "LOSS: 9.501010458933758\n",
      "LOSS: 9.49030235813191\n",
      "LOSS: 9.517365548500726\n",
      "LOSS: 9.483015596504321\n",
      "LOSS: 9.460231108617208\n",
      "LOSS: 9.475900577222625\n",
      "LOSS: 9.491563038320383\n",
      "LOSS: 9.464127487720214\n",
      "LOSS: 9.507830934434937\n",
      "LOSS: 9.440876274849611\n",
      "LOSS: 9.449253933480005\n",
      "LOSS: 9.433859486885172\n",
      "LOSS: 9.470023296217413\n",
      "LOSS: 9.45864942965109\n",
      "LOSS: 9.480378556163087\n",
      "LOSS: 9.451862149205546\n",
      "LOSS: 9.459880191044268\n",
      "LOSS: 9.445083909307336\n",
      "LOSS: 9.413562093653049\n",
      "LOSS: 9.420420739919852\n",
      "LOSS: 9.434706015957756\n",
      "LOSS: 9.427896111780138\n",
      "LOSS: 9.43047763574525\n",
      "LOSS: 9.453575740463444\n",
      "LOSS: 9.407241117622181\n",
      "LOSS: 9.434561047048538\n",
      "LOSS: 9.397820243575051\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train, x_test,y_test = load_mnist(\"data/mnist_train.csv\",\"data/mnist_test.csv\" )\n",
    "\n",
    "batch_size = 20 \n",
    "in_neurons = x_train.shape[1]\n",
    "hid_layer_neurons = 40 \n",
    "iterations = 200\n",
    "learning_rate = 0.00003 \n",
    "num_class = 10\n",
    "\n",
    "\n",
    "# one hot encoding labels \n",
    "y_train = np.eye(10)[y_train]\n",
    "y_train = y_train.reshape(y_train.shape[0],y_train.shape[2])\n",
    "\n",
    "Loss_list, weight_mat1, weight_mat2, bias1, bias2 = training_loop(x_train, y_train,batch_size, iterations, num_class,hid_layer_neurons,in_neurons )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def learning_curve_plot(Loss):\n",
    "    \"\"\"\n",
    "    plotting learning curve \n",
    "    \n",
    "    inputs: list containing loss per epoch \n",
    "    outputs: learning curve plot \n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(1,iterations+1,1)\n",
    "    plt.plot(x,Loss)\n",
    "    plt.xlabel(\"epochs/iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "learning_curve_plot(Loss_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
