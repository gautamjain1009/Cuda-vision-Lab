{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3d34480f90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gautam Jain, Jannis Horn \n",
    "\n",
    "%matplotlib notebook\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as topt\n",
    "import wandb\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.manual_seed( 666 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hinge loss\n",
    "def hingeLoss( pred, label ):\n",
    "\n",
    "# softmax\n",
    "\n",
    "def softmaxLoss( pred, label ):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Input Shape: torch.Size([3, 32, 32])\n",
      "Training Set: 50000, Test Set: 10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "trainset = dsets.CIFAR10('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = dsets.CIFAR10('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "print( \"Input Shape: {}\".format( trainset[0][0].shape ) )\n",
    "print( \"Training Set: {}, Test Set: {}\".format( len(trainset), len(testset) ) )\n",
    "\n",
    "#print( trainset[0][0] )\n",
    "\n",
    "dim_in = 3*32*32\n",
    "dim_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logTraining( epoch, train_loss, time ):\n",
    "    wandb.log( {\"train_loss\": train_loss, \"train_time\": time}, step=epoch )\n",
    "    \n",
    "def logTest( epoch, test_loss, acc, conf_mat, time ):\n",
    "    wandb.log( {\"test_loss\": test_loss,\n",
    "                \"accuracy\": acc, \n",
    "                \"conf_mat\": [wandb.Image(conf_mat, caption=\"Confussion Matrix\")],\n",
    "                \"test_time\": time},\n",
    "               step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinRegClassifier(\n",
      "  (layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=3072, out_features=128, bias=True)\n",
      "      (1): Dropout(p=0.2, inplace=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): Dropout(p=0.2, inplace=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer_out): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LinRegClassifier( nn.Module ):\n",
    "    \n",
    "    def __init__( self, layer_list, dropout, act ):\n",
    "        super( LinRegClassifier, self ).__init__()\n",
    "        self.act_f = self.stringToFunc( act )\n",
    "        self.dropout = dropout\n",
    "        self.buildNet( layer_list )\n",
    "        \n",
    "    def stringToFunc( self, act_str ):\n",
    "        if act_str in [\"relu\", \"ReLU\"]:\n",
    "            return nn.ReLU\n",
    "        elif act_str in [\"sig\", \"sigmoid\", \"Sigmoid\"]:\n",
    "            return nn.Sigmoid\n",
    "        elif act_str in [\"tanh\", \"Tanh\"]:\n",
    "            return nn.Tanh\n",
    "        else:\n",
    "            raise RuntimeError( \"Unknown act_str\" )\n",
    "        \n",
    "    def buildNet( self, layer_list ):\n",
    "        self.layers = nn.ModuleList()\n",
    "        for it, dim_in in enumerate(layer_list[:-2]):\n",
    "            self.layers.append( self.buildLayer( dim_in, layer_list[it+1], self.dropout, self.act_f ) )\n",
    "        self.layer_out = nn.Linear( layer_list[-2], layer_list[-1] )\n",
    "            \n",
    "    def buildLayer( self, dim_in, dim_out, dropout, act_f ):\n",
    "        return nn.Sequential( nn.Linear( dim_in, dim_out ),\n",
    "                              nn.Dropout( dropout ),\n",
    "                              act_f() )\n",
    "    \n",
    "    def save( self, f ):\n",
    "        torch.save(self.state_dict(), \"{}.th\".format(f))\n",
    "        \n",
    "    def load( self, f ):\n",
    "        self.load_state_dict(torch.load( \"{}.th\".format(f) ))\n",
    "        \n",
    "    \n",
    "    def forward( self, x ):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.layer_out(x)\n",
    "    \n",
    "    \n",
    "    def getGradientNorm( self ):\n",
    "        out = []\n",
    "        for p in self.parameters():\n",
    "            out.append( p.data.norm(2) )\n",
    "        return out\n",
    "    \n",
    "    def getL1Norm( self ):\n",
    "        n = torch.tensor( 0.0 ).to( device )\n",
    "        for l in self.layers:\n",
    "            n += l[0].weight.abs().sum()\n",
    "        n += self.layer_out.weight.abs().sum()\n",
    "        return n\n",
    "    \n",
    "    def getL2Norm( self ):\n",
    "        n = torch.tensor( 0.0 ).to( device )\n",
    "        for l in self.layers:\n",
    "            n += l[0].weight.square().sum()\n",
    "        n += self.layer_out.weight.square().sum()\n",
    "        return n\n",
    "    \n",
    "    def wandbConfig( self ):\n",
    "        wandb.config.act_f = str(self.act_f())\n",
    "        wandb.config.dropout = self.dropout\n",
    "        wandb.config.ct_layers = len(self.layers) +1\n",
    "        for it, layer in enumerate(self.layers):\n",
    "            wandb.config.update( {\"layer_{}\".format(it): str(layer[0].out_features)} )\n",
    "        \n",
    "    \n",
    "layers = [dim_in,128,128,dim_out]\n",
    "dropout = 0.2\n",
    "act = \"relu\"\n",
    "\n",
    "Net = LinRegClassifier( layers, dropout, act )\n",
    "print(Net)\n",
    "#print(Net.getGradientNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    def wandbConfig( self ):\n",
    "        self.model.wandbConfig()\n",
    "        wandb.config.optimizer = self.optimizer\n",
    "        wandb.config.regularizer = \"lambdas={}\".format( self.reg_w )\n",
    "        wandb.config.dataset = \"Cifar10\"\n",
    "    \n",
    "    \n",
    "    def initRun( self, model, batch_size, dev, opt, lr, momentum, reg ):\n",
    "        self.model = model\n",
    "        self.dev = dev\n",
    "        self.model.to( dev )\n",
    "        if opt in [topt.Adagrad, topt.Adadelta, topt.Adam]:\n",
    "            self.optimizer = opt( model.parameters(), lr=lr )\n",
    "        else:\n",
    "            self.optimizer = opt( model.parameters(), lr=lr, momentum=momentum )\n",
    "        self.loss_func = nn.CrossEntropyLoss().to( dev )\n",
    "        self.reg_w = torch.Tensor([reg[0], reg[1]]).to( dev )\n",
    "        self.tr_size = len( trainset )\n",
    "        self.te_size = len( testset )\n",
    "        self.loader_training = torch.utils.data.DataLoader( dataset=trainset, \n",
    "                                                            batch_size=batch_size, \n",
    "                                                            shuffle=True,\n",
    "                                                            num_workers=2 )\n",
    "        self.loader_eval = torch.utils.data.DataLoader( dataset=testset, \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        shuffle=False,\n",
    "                                                        num_workers=2 )\n",
    "            \n",
    "    \n",
    "    def testModel( self ):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            st_pt = time.time()\n",
    "            conv_mat = np.zeros([10,10])\n",
    "            corr = 0\n",
    "            bt_loss = 0.0\n",
    "            for (x, label) in self.loader_eval:\n",
    "                x = x.view( -1, dim_in ).to( self.dev )\n",
    "                label = label.to( self.dev )\n",
    "                out = self.model(x)\n",
    "                loss = self.loss_func( out, label )\n",
    "                bt_loss += loss.cpu().item()\n",
    "                \n",
    "                _, pred = torch.max( out.data, 1 )\n",
    "                ls = label.cpu().numpy()\n",
    "                pr = pred.cpu().numpy()\n",
    "                for it in range( label.size(0) ):\n",
    "                    conv_mat[ls[it], pr[it]] += 1\n",
    "                corr += (pred.cpu() == label.cpu()).sum().float()\n",
    "                \n",
    "            ts_time = time.time() -st_pt\n",
    "        return bt_loss /len( self.loader_eval ), corr/self.te_size, 10*conv_mat/self.te_size, ts_time\n",
    "\n",
    "        \n",
    "    def trainModel( self ):\n",
    "        self.model.train()\n",
    "        st_pt = time.time()\n",
    "        bt_loss = 0.0\n",
    "        for it, (x, label) in enumerate( self.loader_training ):\n",
    "            self.optimizer.zero_grad()\n",
    "            x = x.view(-1,dim_in).to( self.dev )\n",
    "            label = label.to( self.dev )\n",
    "\n",
    "            out = self.model(x)\n",
    "            loss = self.loss_func(out, label)\n",
    "            norm_1 = self.model.getL1Norm()\n",
    "            norm_2 = self.model.getL2Norm()\n",
    "            loss += self.reg_w[0] *norm_1 + self.reg_w[1] *norm_2\n",
    "            loss.backward()\n",
    "            bt_loss += loss.cpu().item()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "        tr_time = time.time() -st_pt\n",
    "        return bt_loss /len( self.loader_training ), tr_time\n",
    "    \n",
    "    \n",
    "    def __call__( self, model, config, num_epoch, key=\"\" ):\n",
    "        run = wandb.init( project=\"cudavision3\", name=key, reinit=True )\n",
    "        with run:\n",
    "            self.initRun( model, config[\"bs\"], config[\"dev\"], \n",
    "                          config[\"opt\"], config[\"lr\"], config[\"mom\"],\n",
    "                          config[\"reg\"] )\n",
    "            self.wandbConfig()\n",
    "            for epoch in range(num_epoch):\n",
    "                tr_l, tr_time = self.trainModel()\n",
    "                logTraining( epoch, tr_l, tr_time )\n",
    "                ts_l, acc, cmat, ts_time = self.testModel()\n",
    "                logTest( epoch, ts_l, acc, cmat, ts_time )\n",
    "        del model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD, Adam, Adagrad, Adadelta, RMSprop\n",
    "opts = [topt.Adam, topt.Adagrad, topt.Adadelta, topt.SGD, topt.RMSprop]\n",
    "\n",
    "dropouts = [0,0.2,0.5]\n",
    "act = [\"relu\", \"sig\", \"tan\"]\n",
    "#layers = [dim_in,1024,1024,1024,768,512,512,dim_out]\n",
    "layers = [dim_in,256,256,256,256,256,dim_out]\n",
    "\n",
    "cfg = { \"bs\": 1000, \"dev\": device, \"opt\": opts[0], \n",
    "        \"lr\": 5*10e-4, \"mom\": 0.5, \"reg\": (10e-6, 10e-5) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">optimizer_0</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/thehorn93/cudavision3\" target=\"_blank\">https://wandb.ai/thehorn93/cudavision3</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/thehorn93/cudavision3/runs/3mftcp47\" target=\"_blank\">https://wandb.ai/thehorn93/cudavision3/runs/3mftcp47</a><br/>\n",
       "                Run data is saved locally in <code>/home/user/horn/Documents/wandb/run-20201204_131419-3mftcp47</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17677<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.07MB of 0.07MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/user/horn/Documents/wandb/run-20201204_131419-3mftcp47/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/user/horn/Documents/wandb/run-20201204_131419-3mftcp47/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss</td><td>1.7805</td></tr><tr><td>train_time</td><td>2.18514</td></tr><tr><td>test_loss</td><td>1.63721</td></tr><tr><td>accuracy</td><td>0.4212</td></tr><tr><td>test_time</td><td>0.51447</td></tr><tr><td>_step</td><td>399</td></tr><tr><td>_runtime</td><td>1073</td></tr><tr><td>_timestamp</td><td>1607085132</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss</td><td>█▅▂▃▃▂▂▂▂▁▁▁▂▂▁▂▂▂▁▁▂▁▂▂▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁</td></tr><tr><td>train_time</td><td>▆▆▅▅█▅▆▆▆▆█▆▁▃▃▆▆▁▄▃▅▅█▄▂▅█▄▄▅▄▄▅▅▆▆▆▇▁▅</td></tr><tr><td>test_loss</td><td>█▄▃▂▃▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▁▃▂▁▂▂▂▂▂▁▁▂▂▂▂▂▂</td></tr><tr><td>accuracy</td><td>▁▅▇▇▆▇██▇████▇▇▇▇▇▇▇▇███▇▇█▇▇▇▇▇▇██▇█▇▇█</td></tr><tr><td>test_time</td><td>▅▄▆▄▄▃▅▇▆▆▂▅▂▄▄▂▆▁▂▁▅▃█▂▂▅▄▃▂▄█▄█▆▃▃▆▄▃▆</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 400 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">optimizer_0</strong>: <a href=\"https://wandb.ai/thehorn93/cudavision3/runs/3mftcp47\" target=\"_blank\">https://wandb.ai/thehorn93/cudavision3/runs/3mftcp47</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">optimizer_1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/thehorn93/cudavision3\" target=\"_blank\">https://wandb.ai/thehorn93/cudavision3</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/thehorn93/cudavision3/runs/1lwe80fc\" target=\"_blank\">https://wandb.ai/thehorn93/cudavision3/runs/1lwe80fc</a><br/>\n",
       "                Run data is saved locally in <code>/home/user/horn/Documents/wandb/run-20201204_133215-1lwe80fc</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model = LinRegClassifier( layers, dropouts[1], act[0] )\n",
    "test_model.save( \"test_model\" )\n",
    "runner = Runner()\n",
    "test_epochs = 400\n",
    "for it, opt in enumerate(opts):\n",
    "    model = LinRegClassifier( layers, dropouts[1], act[0] )\n",
    "    model.load( \"test_model\" )\n",
    "    cfg[\"opt\"] = opt\n",
    "    runner( model, cfg, test_epochs, key=\"optimizer_{}\".format(it) )\n",
    "cfg[\"opt\"] = opts[0]\n",
    "    \n",
    "for it, a in enumerate(act[1:]):\n",
    "    model = LinRegClassifier( layres, dropouts[1], a )\n",
    "    model.load( \"test_model\" )\n",
    "    runner( model, cfg, test_epochs, key=\"act_f_{}\".format(it) )    \n",
    "\n",
    "for it, dps in enumerate(dropouts):\n",
    "    model = LinRegClassifier( layres, dps, act[0] )\n",
    "    model.load( \"test_model\" )\n",
    "    runner( model, cfg, test_epochs, key=\"dropout_{}\".format(it) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a78104997e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinRegClassifier\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "runner = Runner()\n",
    "cfg = { \"bs\": 10000, \"dev\": device, \"opt\": opts[2], \n",
    "        \"lr\": 10e-5, \"mom\": 0.5, \"reg\": regs[0] }\n",
    "\n",
    "layers=[dim_in,2048,2048,1024,1024,256,256,64,dim_out]\n",
    "model = LinRegClassifier( layers, dropouts[1], act[0] )\n",
    "runner( model, cfg, 1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
