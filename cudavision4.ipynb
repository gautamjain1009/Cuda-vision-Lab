{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0083548a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gautam Jain, Jannis Horn \n",
    "\n",
    "#%matplotlib notebook\n",
    "import time\n",
    "from typing import Tuple\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as topt\n",
    "import wandb\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.manual_seed( 666 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Input Shape: torch.Size([3, 32, 32])\n",
      "Training Set: 50000, Test Set: 10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "trainset = dsets.CIFAR10('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = dsets.CIFAR10('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "print( \"Input Shape: {}\".format( trainset[0][0].shape ) )\n",
    "print( \"Training Set: {}, Test Set: {}\".format( len(trainset), len(testset) ) )\n",
    "\n",
    "#print( trainset[0][0] )\n",
    "\n",
    "dim_in = tuple(trainset[0][0].shape)\n",
    "dim_out = 10\n",
    "dt_dims = (dim_in, dim_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "current_cmap = copy.copy( matplotlib.cm.get_cmap() )\n",
    "current_cmap.set_bad(color='white')\n",
    "\n",
    "def flattenConvVols( vol ):\n",
    "    \"\"\" Flatten 3D Volume along 0 axis and concat with whitespace \"\"\"\n",
    "    m = int(np.ceil(np.sqrt( vol.shape[0] )))\n",
    "    out = torch.full( [vol.shape[1]*m+m-1, vol.shape[2]*m+m-1], np.nan )\n",
    "    x_it = 0\n",
    "    y_it = 0\n",
    "    for it in range( vol.shape[0] ):\n",
    "        x_st = x_it *vol.shape[1] +x_it\n",
    "        y_st = y_it *vol.shape[2] +y_it\n",
    "        out[x_st:x_st+vol.shape[1],y_st:y_st+vol.shape[2]] = vol[it,:,:]\n",
    "        y_it += 1\n",
    "        if y_it >= m:\n",
    "            y_it = 0\n",
    "            x_it += 1\n",
    "    return out\n",
    "\n",
    "def flattenConvWeight( vol ):\n",
    "    \"\"\" Flatten 4D Volume to rows to 2D \"\"\"\n",
    "    out = torch.full( [(vol.shape[2]+3)*vol.shape[0]-1, (vol.shape[3]+1)*vol.shape[1]-1], np.nan )\n",
    "    for x_it in range( vol.shape[0] ):\n",
    "        for y_it in range( vol.shape[1] ):\n",
    "            x_st = x_it *vol.shape[2] +3*x_it\n",
    "            y_st = y_it *vol.shape[3] +y_it\n",
    "            out[x_st:x_st+vol.shape[2],y_st:y_st+vol.shape[3]] = vol[x_it,y_it,:,:]\n",
    "    return out\n",
    "    \n",
    "\n",
    "def convVolsAsFigure( vol ):\n",
    "    \"\"\" Create 2D Image from 3D input volume and add a colorbar \"\"\"\n",
    "    m = int(np.ceil(np.sqrt( vol.shape[0] )))\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    if vol.dim() == 3: \n",
    "        a = flattenConvVols( vol )\n",
    "        fig.set_size_inches( 0.075*a.shape[1]+2, 0.075*a.shape[0] )\n",
    "    elif vol.dim() == 4: \n",
    "        a = flattenConvWeight( vol )\n",
    "        fig.set_size_inches( 0.15*a.shape[1]+2, 0.15*a.shape[0] )\n",
    "    im = ax.imshow( a, cmap=current_cmap )\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.1)\n",
    "    fig.colorbar( im, cax=cax )\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logTraining( epoch, train_loss, time ):\n",
    "    \"\"\" Log training results \"\"\"\n",
    "    wandb.log( {\"train_loss\": train_loss, \n",
    "                \"train_time\": time}, step=epoch )\n",
    "    \n",
    "def logTest( epoch, test_loss, acc, conf_mat, time ):\n",
    "    \"\"\" Log information gathered from testing \"\"\"\n",
    "    wandb.log( {\"test_loss\": test_loss,\n",
    "                \"accuracy\": acc, \n",
    "                \"conf_mat\": [wandb.Image(conf_mat, caption=\"Confussion Matrix\")],\n",
    "                \"test_time\": time},\n",
    "               step=epoch)\n",
    "    \n",
    "def logNetData( epoch, acts, weights, grads ):\n",
    "    \"\"\" Log images for net activation, weights and grads.\n",
    "    \n",
    "    For each element in given dictionaries create a figure and log image\n",
    "    \"\"\"\n",
    "    ims = {}\n",
    "    for k, a in acts.items():\n",
    "        f = convVolsAsFigure(a[0,:,:,:])\n",
    "        ims[\"act_{}\".format(k)] = wandb.Image(f, caption=k)\n",
    "        plt.close(f)\n",
    "    for k, w in weights.items():\n",
    "        f = convVolsAsFigure(w[:,:,:,:])\n",
    "        ims[\"w_{}\".format(k)] = wandb.Image(f, caption=k)\n",
    "        plt.close(f)\n",
    "    for k, g in grads.items():\n",
    "        f = convVolsAsFigure(g[:,:,:,:])\n",
    "        ims[\"grad_{}\".format(k)] = wandb.Image(f, caption=k)\n",
    "        plt.close(f)\n",
    "    wandb.log( ims, step=epoch )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hook taken from https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/6\n",
    "\n",
    "class MaxAvgPool2d( nn.Module ):\n",
    "    def __init__( self, ksize, stride=None, padding=0 ):\n",
    "        super( MaxAvgPool2d, self ).__init__()\n",
    "        self.mpool = nn.MaxPool2d( ksize, stride, padding )\n",
    "        self.apool = nn.AvgPool2d( ksize, stride, padding )\n",
    "        self.kernel_size = ksize\n",
    "    \n",
    "    def forward( self, x ):\n",
    "        x1 = self.mpool( x )\n",
    "        x2 = self.apool( x )\n",
    "        return torch.cat((x1, x2), dim=1)\n",
    "\n",
    "\n",
    "    \n",
    "class ConvNet( nn.Module ):\n",
    "    \"\"\"\n",
    "    Wrapper for convolutional neural network.\n",
    "    \n",
    "    Holds layers, hooks for interlayer inputs/outputs.\n",
    "    Computes norms and creates wandb config\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, c_layers, l_layers, map_size ):\n",
    "        super( ConvNet, self ).__init__()\n",
    "        self.conv_layers = c_layers\n",
    "        self.lin_layers = l_layers\n",
    "        self.lin_inp_dim = l_layers[0][0].in_features\n",
    "        self.map_size = map_size\n",
    "        self.gradients = {}\n",
    "        self.activations = {}\n",
    "        self.weights = {}\n",
    "        self.collect = False\n",
    "        \n",
    "    \n",
    "    def save( self, f ):\n",
    "        torch.save(self.state_dict(), \"{}.th\".format(f))\n",
    "        \n",
    "    def load( self, f ):\n",
    "        self.load_state_dict(torch.load( \"{}.th\".format(f) ))\n",
    "        \n",
    "    def hook( self, hook, seq, name ):\n",
    "        def getActivationHook( model, input, output ):\n",
    "            if self.collect: self.activations[name] = output.detach().to( \"cpu\" )\n",
    "        def getWeightHook( model, input, output ):\n",
    "            if self.collect: self.weights[name] = model.weight.detach().to( \"cpu\" )\n",
    "        def getGradientHook( model, grad_input, grad_output ):\n",
    "            if self.collect: self.gradients[name] = grad_output.detach().to( \"cpu\" )\n",
    "        if hook in [1,3,5,7]:\n",
    "            seq[0].register_forward_hook( getActivationHook )\n",
    "        if hook in [2,3,6,7]:\n",
    "            seq[0].register_backward_hook( getGradientHook )\n",
    "        if hook in [4,5,6,7]:\n",
    "            seq[0].register_forward_hook( getWeightHook )\n",
    "        \n",
    "    def hookAll( self, hooks ):\n",
    "        for it, l in enumerate(self.conv_layers):\n",
    "            self.hook( hooks[0], l, \"conv{}\".format(it) )\n",
    "        for it, l in enumerate(self.conv_layers):\n",
    "            self.hook( hooks[1], l, \"lin{}\".format(it) )\n",
    "        \n",
    "    \n",
    "    def forward( self, x ):\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        x = x.view( -1, self.lin_inp_dim )\n",
    "        for layer in self.lin_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def getL1Norm( self ):\n",
    "        n = torch.tensor( 0.0 ).to( device )\n",
    "        for l in self.conv_layers:\n",
    "            n += l[0].weight.abs().sum()\n",
    "        for l in self.lin_layers[:-1]:\n",
    "            n += l[0].weight.abs().sum()\n",
    "        n += self.lin_layers[-1].weight.abs().sum()\n",
    "        return n\n",
    "    \n",
    "    def getL2Norm( self ):\n",
    "        n = torch.tensor( 0.0 ).to( device )\n",
    "        for l in self.conv_layers:\n",
    "            n += l[0].weight.square().sum()\n",
    "        for l in self.lin_layers[:-1]:\n",
    "            n += l[0].weight.square().sum()\n",
    "        n += self.lin_layers[-1].weight.square().sum()\n",
    "        return n\n",
    "    \n",
    "    \n",
    "    def getGradientNorm( self, norm=2 ):\n",
    "        out = {}\n",
    "        for key, val in self.gradients:\n",
    "            out[key] = val.norm( norm )\n",
    "            \n",
    "    def getActivations( self ):\n",
    "        return self.activations\n",
    "    \n",
    "    def getWeights( self, key ):\n",
    "        return self.weights\n",
    "    \n",
    "    def paramOut( self ):\n",
    "        self.collect = True\n",
    "    \n",
    "    def wandbConfig( self ):\n",
    "        wandb.config.conv_layers = len(self.conv_layers)\n",
    "        wandb.config.lin_layers = len(self.lin_layers)\n",
    "        for it, seq in enumerate(self.conv_layers):\n",
    "            self.convToParamSet( it, seq )\n",
    "        for it, seq in enumerate(self.lin_layers):\n",
    "            self.linToParamSet( it, seq )\n",
    "            \n",
    "    def convToParamSet( self, it, seq ):\n",
    "        def wandbConfig( name, it, key, val ):\n",
    "            wandb.config.update( {\"{}{}_{}\".format(name, it, key): val} )\n",
    "            \n",
    "        for name, mod in seq.named_modules():\n",
    "            if \"conv\" in name:\n",
    "                wandbConfig( name, it, \"ch_out\", mod.out_channels )\n",
    "                wandbConfig( name, it, \"ksize\", mod.kernel_size )\n",
    "                wandbConfig( name, it, \"stride\", mod.stride )\n",
    "                wandbConfig( name, it, \"pad\", mod.padding )\n",
    "            elif \"pool\" in name:\n",
    "                if isinstance( mod, nn.MaxPool2d ): pt = \"max\"\n",
    "                elif isinstance( mod, nn.AvgPool2d ): pt = \"avg\"\n",
    "                elif isinstance( mod, MaxAvgPool2d ): pt = \"both\"\n",
    "                wandbConfig( name, it, \"pool_type\", pt )\n",
    "                wandbConfig( name, it, \"pool_ks\", mod.kernel_size )\n",
    "            elif \"drop\" in name:\n",
    "                wandb.config.update( {\"{}_conv{}\".format(name,it): mod.p} )\n",
    "            elif \"act\" in name:\n",
    "                wandb.config.update( {\"{}_conv{}\".format(name,it): mod} )\n",
    "                \n",
    "    def linToParamSet( self, it, seq ):\n",
    "        for name, mod in seq.named_modules():\n",
    "            if \"lin\" in name:\n",
    "                wandb.config.update( {\"{}{}\".format(name,it): mod.out_features} )\n",
    "            if \"act\" in name:\n",
    "                wandb.config.update( {\"{}_lin{}\".format(name,it): mod} )\n",
    "            if \"drop\" in name:\n",
    "                wandb.config.update( {\"{}_lin{}\".format(name,it): mod.p} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (conv): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (lin_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (1): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ") [(3, 32, 32), (16, 16.0, 16.0), (16, 8.0, 8.0), (16, 4.0, 4.0), 256, 10]\n",
      "ConvNet(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (lin_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ") [(3, 32, 32), (16, 16.0, 16.0), (32, 8.0, 8.0), (16, 4.0, 4.0), 256, 256, 10]\n"
     ]
    }
   ],
   "source": [
    "class Config():\n",
    "    \"\"\"\n",
    "    Wrapper to convert lists or dictionaries to constructor tokens\n",
    "    \n",
    "    Parse lists and dictionaries of different parameters\n",
    "    Generate tokenset for unified construction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dt_dims ):\n",
    "        super().__init__()\n",
    "        self.dts_dims = dt_dims\n",
    "        self.num_c = 0\n",
    "        self.num_l = 0\n",
    "        self.ps_c = []\n",
    "        self.ps_l = []\n",
    "\n",
    "    @classmethod \n",
    "    def fromList( cls, dt_dims, ilist ):\n",
    "        out = cls( dt_dims )\n",
    "        out.parseList( ilist )\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def fromDict( cls, dt_dims, idict ):\n",
    "        out = cls( dt_dims )\n",
    "        out.parseDict( idict )\n",
    "        return out\n",
    "\n",
    "    def parseDict( self, idict ):\n",
    "        def ifInDict( key, default ):\n",
    "            if key in idict:\n",
    "                return idict[key]\n",
    "            else:\n",
    "                return default\n",
    "\n",
    "        act_f = ifInDict( \"act\", \"relu\" )\n",
    "        dp_f = ifInDict( \"dp\", 0.0 )\n",
    "        if \"num_c\" in idict:\n",
    "            out_c = ifInDict( \"out_c\", None )\n",
    "            ksize_c = ifInDict( \"ksize\", None )\n",
    "            act_c = ifInDict( \"act_c\", act_f )\n",
    "            stride_c = ifInDict( \"stride\", 1 )\n",
    "            pad_c = ifInDict( \"pad\", False )\n",
    "            pool_c = ifInDict( \"pool\", (\"max\", 1) )\n",
    "            dp_c = ifInDict( \"dp_c\", dp_f )\n",
    "            for it in range(idict[\"num_c\"]):\n",
    "                out = ifInDict( \"out_c_{}\".format(it), out_c )\n",
    "                ksize = ifInDict( \"ksize_{}\".format(it), ksize_c )\n",
    "                act = ifInDict( \"act_c_{}\".format(it), act_c )\n",
    "                stride = ifInDict( \"stride_{}\".format(it), stride_c )\n",
    "                pad = ifInDict( \"pad_{}\".format(it), pad_c )\n",
    "                pool = ifInDict( \"pool_{}\".format(it), pool_c )\n",
    "                dp = ifInDict( \"dp_c_{}\".format(it), dp_c )\n",
    "                self.addConvLayer( out, ksize, act, stride, pad, pool, dp )\n",
    "        if \"num_l\" in idict:\n",
    "            out_l = ifInDict( \"out_l\", None )\n",
    "            act_l = ifInDict( \"act_l\", act_f )\n",
    "            dp_l = ifInDict( \"dp_l\", dp_f )\n",
    "            for it in range( idict[\"num_l\"] ):\n",
    "                out = ifInDict( \"out_l_{}\", out_l )\n",
    "                act = ifInDict( \"act_l_{}\", act_l )\n",
    "                dp_l = ifInDict( \"dp_l_{}\", dp_l )\n",
    "                self.addLinearLayer( out, act, dp_l )\n",
    "\n",
    "\n",
    "    def parseList( self, ilist ):\n",
    "        def ifIt( obj, it, default ):\n",
    "            if len(obj) > it: return obj[it]\n",
    "            else: return default\n",
    "\n",
    "        for tp in ilist:\n",
    "            #Convolutional Parameters: [\"c\",ch_out,ksize,act_str,stride*,pad*,pool*,dropout*]\n",
    "            if tp[0] == \"c\":\n",
    "                self.addConvLayer( tp[1], tp[2], tp[3], ifIt(tp,4,1), ifIt(tp,5,0),\n",
    "                                   ifIt(tp,6,(\"max\",1)), ifIt(tp,7,0.0) )\n",
    "            #Linear Parameters: [\"l\",size_out,act_str,dropout*]\n",
    "            elif tp[0] == \"l\":\n",
    "                self.addLinearLayer( tp[1], tp[2], ifIt(tp,3,0.0) )\n",
    "\n",
    "\n",
    "\n",
    "    def addConvLayer( self, ch_out: int, k_s: int, act: str, stride: int, pad: bool, \n",
    "                      pool: Tuple[str, int], dropout: float ):\n",
    "        self.num_c += 1\n",
    "        self.ps_c.append( {\"out\":ch_out, \"ksize\":k_s, \"act\":act, \"str\":stride, \n",
    "                           \"pad\":pad, \"pool\":pool, \"dp\":dropout} )\n",
    "\n",
    "    def addLinearLayer( self, s_out: int, act: str, dropout: int ):\n",
    "        self.num_l += 1\n",
    "        self.ps_l.append( {\"out\":s_out, \"act\":act, \"dp\":dropout} )\n",
    "\n",
    "        \n",
    "    def changeConv( self, key, params ):\n",
    "        if key in self.ps_c[0]:\n",
    "            for it, p in enumerate(params):\n",
    "                if p is not None:\n",
    "                    self.ps_c[it][key] = p\n",
    "                \n",
    "    def changeLin( self, key, params ):\n",
    "        if key in self.ps_l[0]:\n",
    "            for it, p in enumerate(params):\n",
    "                if p is not None:\n",
    "                    self.ps_l[it][key] = p\n",
    "        \n",
    "\n",
    "    def __getitem__( self, it ):\n",
    "        if it < self.num_c:\n",
    "            return self.ps_c[it]\n",
    "        else:\n",
    "            return self.ps_l[it-num_c] \n",
    "\n",
    "\n",
    "class Constructor:\n",
    "    \"\"\"\n",
    "    Given config create layers for ConvNet\n",
    "    \"\"\"\n",
    "    def __call__( self, net_type, config ):\n",
    "        conv_layers = nn.ModuleList()\n",
    "        lin_layers = nn.ModuleList()\n",
    "        map_size = []\n",
    "        dim_in = config.dts_dims[0]\n",
    "        map_size.append( dim_in )\n",
    "        for ps in config.ps_c:\n",
    "            l, dim_in = self.buildConvLayer( dim_in, ps )\n",
    "            conv_layers.append( l )\n",
    "            map_size.append( dim_in )\n",
    "        dim_in = np.array(dim_in, dtype=np.int).prod().item()\n",
    "        for ps in config.ps_l:\n",
    "            l, dim_in = self.buildLinLayer( dim_in, ps )\n",
    "            lin_layers.append( l )\n",
    "            map_size.append( dim_in )\n",
    "        lin_layers.append( nn.Linear( dim_in, config.dts_dims[1] ) )\n",
    "        map_size.append( config.dts_dims[1] )\n",
    "        return net_type( conv_layers, lin_layers, map_size )\n",
    "\n",
    "    def buildConvLayer( self, d_in, params ):\n",
    "        out = OrderedDict()\n",
    "        if params[\"pad\"]: pad = int((params[\"ksize\"]-1) /2)\n",
    "        else: pad = 0\n",
    "        out[\"conv\"] = nn.Conv2d( d_in[0], params[\"out\"], params[\"ksize\"], params[\"str\"], padding=pad )\n",
    "        if params[\"dp\"] > 0.0:\n",
    "            out[\"drop\"] = nn.Dropout( params[\"dp\"] )\n",
    "        pl = params[\"pool\"]\n",
    "        m = 1\n",
    "        if pl[1] > 1:\n",
    "            pool, m = self.strToPool( pl[0] )\n",
    "            out[\"pool\"] = pool( pl[1] )\n",
    "        out[\"act\"] = self.strToAct( params[\"act\"] )()\n",
    "        toNewSize = lambda x : (x-params[\"ksize\"]+1 +pad*2) /params[\"str\"] / pl[1]\n",
    "        d_out = (params[\"out\"]*m, toNewSize(d_in[1]), toNewSize(d_in[2]))\n",
    "        return nn.Sequential( out ), d_out\n",
    "\n",
    "    def buildLinLayer( self, s_in, params ):\n",
    "        out = OrderedDict()\n",
    "        out[\"lin\"] = nn.Linear( s_in, params[\"out\"] )\n",
    "        if params[\"dp\"] > 0.0:\n",
    "            out[\"drop\"] = nn.Dropout( params[\"dp\"] )\n",
    "        out[\"act\"] = self.strToAct( params[\"act\"] )()\n",
    "        return nn.Sequential( out ), params[\"out\"]\n",
    "\n",
    "    def strToAct( self, act_str ):\n",
    "        if act_str in [\"relu\", \"ReLU\"]:\n",
    "            return nn.ReLU\n",
    "        elif act_str in [\"sig\", \"sigmoid\", \"Sigmoid\"]:\n",
    "            return nn.Sigmoid\n",
    "        elif act_str in [\"tanh\", \"Tanh\"]:\n",
    "            return nn.Tanh\n",
    "        else:\n",
    "            raise RuntimeError( \"Unknown act_str\" )\n",
    "\n",
    "    def strToPool( self, p_str ):\n",
    "        if p_str == \"max\": \n",
    "            return nn.MaxPool2d, 1\n",
    "        elif p_str == \"avg\": \n",
    "            return nn.AvgPool2d, 1\n",
    "        elif p_str == \"both\":\n",
    "            return MaxAvgPool2d, 2\n",
    "        else:\n",
    "            raise RuntimeError( \"Unknown p_str\" )\n",
    "            \n",
    "llist = [[\"c\",16,5,\"relu\",1,True,(\"max\",2)]]*3 +[[\"l\",256,\"relu\",0.2]]\n",
    "ldict = {\"num_c\": 3, \"num_l\":2, \"act\":\"relu\", \"dp\":0.2, \"ksize\":3, \"pool\":(\"max\",2), \n",
    "         \"out_c\":16, \"out_c_1\":32, \"out_l\":256, \"pad\":True}\n",
    "cfg = Config.fromList( dt_dims, llist )\n",
    "cstr = Constructor()\n",
    "test_net = cstr( ConvNet, cfg )\n",
    "test_net_2 = cstr( ConvNet, Config.fromDict(dt_dims,ldict) )\n",
    "print(test_net, test_net.map_size)\n",
    "print(test_net_2, test_net_2.map_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    \"\"\"\n",
    "    Wrapper for network training\n",
    "    \n",
    "    Combines training -> testing and logging\n",
    "    \"\"\"\n",
    "    def __init__( self, project_name, entity=\"cudavisionlab\" ):\n",
    "        self.pr_name = project_name\n",
    "        self.entity = entity\n",
    "    \n",
    "    def wandbConfig( self ):\n",
    "        self.model.wandbConfig()\n",
    "        wandb.config.optimizer = self.optimizer\n",
    "        wandb.config.regularizer = \"lambdas={}\".format( self.reg_w )\n",
    "        wandb.config.dataset = \"Cifar10\"\n",
    "    \n",
    "    \n",
    "    def initRun( self, model, batch_size, dev, opt, lr, momentum, reg ):\n",
    "        self.model = model\n",
    "        self.dev = dev\n",
    "        self.model.to( dev )\n",
    "        if opt in [topt.Adagrad, topt.Adadelta, topt.Adam]:\n",
    "            self.optimizer = opt( model.parameters(), lr=lr )\n",
    "        else:\n",
    "            self.optimizer = opt( model.parameters(), lr=lr, momentum=momentum )\n",
    "        self.loss_func = nn.CrossEntropyLoss().to( dev )\n",
    "        self.reg_w = torch.Tensor([reg[0], reg[1]]).to( dev )\n",
    "        self.tr_size = len( trainset )\n",
    "        self.te_size = len( testset )\n",
    "        self.loader_training = torch.utils.data.DataLoader( dataset=trainset, \n",
    "                                                            batch_size=batch_size, \n",
    "                                                            shuffle=True,\n",
    "                                                            num_workers=2 )\n",
    "        self.loader_eval = torch.utils.data.DataLoader( dataset=testset, \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        shuffle=False,\n",
    "                                                        num_workers=2 )\n",
    "        \n",
    "    def initHooks( self, vis_cfg ):\n",
    "        c_hook, l_hook = 0,0\n",
    "        for c in vis_cfg[\"act\"]:\n",
    "            if c == \"c\": c_hook += 1\n",
    "            elif c == \"l\": l_hook += 1\n",
    "        for c in vis_cfg[\"grad\"]:\n",
    "            if c == \"c\": c_hook += 2\n",
    "            elif c == \"l\": l_hook += 2\n",
    "        for c in vis_cfg[\"weight\"]:\n",
    "            if c == \"c\": c_hook += 4\n",
    "            elif c == \"l\": l_hook += 4\n",
    "        self.model.hookAll( (c_hook, l_hook) )\n",
    "                \n",
    "    \n",
    "    def testModel( self ):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            st_pt = time.time()\n",
    "            conv_mat = np.zeros([10,10])\n",
    "            corr = 0\n",
    "            bt_loss = 0.0\n",
    "            for (x, label) in self.loader_eval:\n",
    "                #x = x.view( -1, dim_in ).to( self.dev )\n",
    "                x = x.to( self.dev )\n",
    "                label = label.to( self.dev )\n",
    "                out = self.model(x)\n",
    "                loss = self.loss_func( out, label )\n",
    "                bt_loss += loss.cpu().item()\n",
    "                \n",
    "                _, pred = torch.max( out.data, 1 )\n",
    "                ls = label.cpu().numpy()\n",
    "                pr = pred.cpu().numpy()\n",
    "                for it in range( label.size(0) ):\n",
    "                    conv_mat[ls[it], pr[it]] += 1\n",
    "                corr += (pred.cpu() == label.cpu()).sum().float()\n",
    "                \n",
    "            ts_time = time.time() -st_pt\n",
    "        return bt_loss /len( self.loader_eval ), corr/self.te_size, 10*conv_mat/self.te_size, ts_time\n",
    "\n",
    "        \n",
    "    def trainModel( self ):\n",
    "        self.model.train()\n",
    "        st_pt = time.time()\n",
    "        bt_loss = 0.0\n",
    "        for it, (x, label) in enumerate( self.loader_training ):\n",
    "            self.optimizer.zero_grad()\n",
    "            #x = x.view(-1,dim_in).to( self.dev )\n",
    "            x = x.to( self.dev )\n",
    "            label = label.to( self.dev )\n",
    "\n",
    "            out = self.model(x)\n",
    "            loss = self.loss_func(out, label)\n",
    "            norm_1 = self.model.getL1Norm()\n",
    "            norm_2 = self.model.getL2Norm()\n",
    "            loss += self.reg_w[0] *norm_1 + self.reg_w[1] *norm_2\n",
    "            loss.backward()\n",
    "            bt_loss += loss.cpu().item()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "        tr_time = time.time() -st_pt\n",
    "        return bt_loss /len( self.loader_training ), tr_time\n",
    "    \n",
    "    \n",
    "    def __call__( self, model, config, vis_cfg, num_epoch, im_int, key=\"\", load=None ):\n",
    "        run = wandb.init( project=self.pr_name, entity=self.entity, name=key, reinit=True )\n",
    "        if load is not None:\n",
    "            model.load( load )\n",
    "        with run:\n",
    "            self.initRun( model, config[\"bs\"], config[\"dev\"], \n",
    "                          config[\"opt\"], config[\"lr\"], config[\"mom\"],\n",
    "                          config[\"reg\"] )\n",
    "            self.initHooks( vis_cfg )\n",
    "            self.wandbConfig()\n",
    "            e_it = 0\n",
    "            for epoch in range(num_epoch):\n",
    "                e_it += 1\n",
    "                if e_it % im_int == 0:\n",
    "                    self.model.collect = True\n",
    "                tr_l, tr_time = self.trainModel()\n",
    "                logTraining( epoch, tr_l, tr_time )\n",
    "                ts_l, acc, cmat, ts_time = self.testModel()\n",
    "                logTest( epoch, ts_l, acc, cmat, ts_time )\n",
    "                if e_it % im_int == 0:\n",
    "                    self.model.collect = False\n",
    "                    logNetData( epoch, self.model.activations, self.model.weights, self.model.gradients )\n",
    "                print( \"{}/{}\".format( e_it, num_epoch ), end='\\r' )\n",
    "        del model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runner = Runner( \"cudavision4\" )\n",
    "cfg = { \"bs\": 10000, \"dev\": device, \"opt\": topt.Adam, \n",
    "        \"lr\": 2*10e-5, \"mom\": 0.5, \"reg\": (10e-5, 10e-4) }\n",
    "vis_cfg = { \"act\":\"c\", \"grad\":\"\", \"weight\":\"c\" }\n",
    "\n",
    "ldict = {\"num_c\": 3, \"num_l\":2, \"act\":\"relu\", \"dp\":0.2, \"ksize\":3, \"pool\":(\"both\",2), \n",
    "         \"out_c\": 32, \"out_c_1\":64, \"out_c_2\":128, \"out_l\":256, \"pad\":True}\n",
    "\n",
    "cstr = Constructor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cstr( ConvNet, Config.fromDict( dt_dims, ldict ) )\n",
    "\n",
    "name = \"c_32_64_128_mreg\"\n",
    "runner( net, cfg, vis_cfg, 3000, 50, name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthehorn93\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">p_both</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/cudavisionlab/cudavision4\" target=\"_blank\">https://wandb.ai/cudavisionlab/cudavision4</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/cudavisionlab/cudavision4/runs/g0x8tqir\" target=\"_blank\">https://wandb.ai/cudavisionlab/cudavision4/runs/g0x8tqir</a><br/>\n",
       "                Run data is saved locally in <code>/home/user/horn/Documents/Cuda-vision-Lab/wandb/run-20201209_125028-g0x8tqir</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3000\r"
     ]
    }
   ],
   "source": [
    "pools = [\"both\"]\n",
    "net_cfg = Config.fromDict( dt_dims, ldict )\n",
    "net_cfg.changeConv( \"out\", [16,16,32] )\n",
    "cfg[\"reg\"] = (10e-6,2*10e-5)\n",
    "\n",
    "for p in pools:\n",
    "    ldict[\"pool\"] = (p,2)\n",
    "    net = cstr( ConvNet, net_cfg )\n",
    "\n",
    "    name = \"p_{}\".format(p)\n",
    "    runner( net, cfg, vis_cfg, 3000, 50, name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save( \"conv_test\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
