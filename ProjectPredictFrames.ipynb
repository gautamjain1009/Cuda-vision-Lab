{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Gautam Jain, Jannis Horn \n",
    "\n",
    "%matplotlib notebook\n",
    "import time\n",
    "import os\n",
    "from typing import Tuple\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as topt\n",
    "import wandb\n",
    "import cv2\n",
    "import os\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.manual_seed( 666 )\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convGRU code taken from: https://github.com/SreenivasVRao/ConvGRU-ConvLSTM-PyTorch/blob/master/convgru.py\n",
    "from convgru import ConvGRU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LadderLayer( nn.Module ):\n",
    "    def __init__( self, w, h, in_channels, out_channels, use_loc_dep ):\n",
    "        super( LadderLayer, self ).__init__()\n",
    "        if not use_loc_dep: self.conv = nn.Conv2d( in_channels, out_channels, (1,1) )\n",
    "        else: self.conv = LocationAwareConv2d( True, False, w, h, in_channels, out_channels, (1,1) )\n",
    "        self.conv_gru3 = ConvGRU( in_channels, out_channels, (3,3), 1, batch_first=True )\n",
    "        self.conv_gru5 = ConvGRU( in_channels, out_channels, (5,5), 1, batch_first=True )\n",
    "        self.conv_gru7 = ConvGRU( in_channels, out_channels, (7,7), 1, batch_first=True )\n",
    "        \n",
    "    def forward( self, x ):\n",
    "        out_l = []\n",
    "        for it in range( x.shape[1] ):\n",
    "            out_l.append( self.conv( x[:,it,:,:,:] ) )\n",
    "        out = torch.stack( out_l, dim=1 )\n",
    "        out3,_ = self.conv_gru3( x )\n",
    "        out5,_ = self.conv_gru5( x )\n",
    "        out7,_ = self.conv_gru7( x )\n",
    "        return torch.cat( [out, out3[0], out5[0], out7[0]], dim=2 )\n",
    "        \n",
    "        \n",
    "class ReconstructionLayer( nn.Module ):\n",
    "    def __init__( self, in_channels, use_btnorm ):\n",
    "        super( ReconstructionLayer, self ).__init__()\n",
    "        if not use_btnorm: self.relu = nn.ReLU()\n",
    "        else: self.relu = nn.Sequential( nn.ReLU(), nn.BatchNorm2d( in_channels ) )\n",
    "        self.conv1 = nn.Conv2d( in_channels, 1024, (3,3), padding=1 )\n",
    "        self.shuffle = nn.PixelShuffle( 2 )\n",
    "        self.conv2 = nn.Conv2d( 256, 64, (3,3), padding=1 )\n",
    "        \n",
    "    def forward( self, x ):\n",
    "        out = self.relu( x )\n",
    "        out = self.conv1( out )\n",
    "        out = self.shuffle( out )\n",
    "        out = self.conv2( out )\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class LocationAwareConv2d(torch.nn.Conv2d):\n",
    "    def __init__(self,locationAware,gradient,w,h,in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        if locationAware:\n",
    "            self.locationBias=torch.nn.Parameter(torch.zeros(w,h,3))\n",
    "            self.locationEncode=torch.autograd.Variable(torch.ones(w,h,3))\n",
    "            if gradient:\n",
    "                for i in range(w):\n",
    "                    self.locationEncode[i,:,1]=self.locationEncode[:,i,0]=i/float(w-1)\n",
    "        \n",
    "        self.up=torch.nn.Upsample(size=(w,h), mode='bilinear', align_corners=False)\n",
    "        self.w=w\n",
    "        self.h=h\n",
    "        self.locationAware=locationAware\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        if self.locationAware:\n",
    "            if self.locationBias.device != inputs.device:\n",
    "                self.locationBias=self.locationBias.to(inputs.get_device())\n",
    "            if self.locationEncode.device != inputs.device:\n",
    "                self.locationEncode=self.locationEncode.to(inputs.get_device())\n",
    "            b=self.locationBias*self.locationEncode\n",
    "        convRes=super().forward(inputs)\n",
    "        if convRes.shape[2]!=self.w and convRes.shape[3]!=self.h:\n",
    "            convRes=self.up(convRes)\n",
    "        if self.locationAware:\n",
    "            return convRes+b[:,:,0]+b[:,:,1]+b[:,:,2]\n",
    "        else:\n",
    "            return convRes\n",
    "        \n",
    "    def  __str__( self ):\n",
    "        return( \"LocationAware{}, LocAware={}, gradient={}\".format( super().__str__(), self.locationAware, self.Gradient ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3, 320, 224])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5f4b474f25ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetL2Norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-5f4b474f25ed>\u001b[0m in \u001b[0;36mgetL2Norm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mladders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "class PredictionModel( nn.Module ):\n",
    "    \n",
    "    def __init__( self, w, h ):\n",
    "        super( PredictionModel, self ).__init__()\n",
    "        self.resnet18 = models.resnet18( pretrained=True )\n",
    "        for param in self.resnet18.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.ladders = nn.ModuleDict( {\n",
    "            \"ladder1\": LadderLayer( int(w/2), int(h/2), 64, 64, False ),\n",
    "            \"ladder2\": LadderLayer( int(w/4), int(h/4), 64, 64, False ),\n",
    "            \"ladder3\": LadderLayer( int(w/8), int(h/8), 128, 64, True ),\n",
    "            \"ladder4\": LadderLayer( int(w/16), int(h/16), 256, 64, True ) \n",
    "        } )\n",
    "        self.recons = nn.ModuleDict( {\n",
    "            \"recon1\": ReconstructionLayer( 64*5, True ),\n",
    "            \"recon2\": ReconstructionLayer( 64*5, True ),\n",
    "            \"recon3\": ReconstructionLayer( 64*5, True ),\n",
    "            \"recon4\": ReconstructionLayer( 512, False )\n",
    "        } )\n",
    "        self.out_conv = nn.Conv2d( 320, 12, (1,1) )\n",
    "        self.out_shuffle = nn.PixelShuffle( 2 )\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def getResnetOutputs( self, x ):\n",
    "        inp_c = self.resnet18.conv1( x )\n",
    "        inp = self.resnet18.bn1( inp_c )\n",
    "        inp = self.resnet18.relu( inp )\n",
    "        inp = self.resnet18.maxpool( inp )\n",
    "        res_out1 = self.resnet18.layer1( inp )\n",
    "        res_out2 = self.resnet18.layer2( res_out1 )\n",
    "        res_out3 = self.resnet18.layer3( res_out2 )\n",
    "        res_out4 = self.resnet18.layer4( res_out3 )\n",
    "        return ( inp_c, res_out1, res_out2, res_out3, res_out4 )\n",
    "    \n",
    "    def getReconstructionOutput( self, res_x, rec_x, l_it ):\n",
    "        l_out = self.ladders[\"ladder{}\".format(l_it)]( res_x )\n",
    "        r_out_l = []\n",
    "        for it in range( rec_x.shape[1] ):\n",
    "            r_out_l.append( self.recons[\"recon{}\".format(l_it)]( rec_x[:,it,:,:,:] ) )\n",
    "        r_out = torch.stack( r_out_l, dim=1 )\n",
    "        out = torch.cat( [r_out, l_out], dim=2 )\n",
    "        return out\n",
    "    \n",
    "    def getL2Norm( self ):\n",
    "        out = torch.tensor(0.0).to( self.out_conv.weight.device )\n",
    "        for param in self.ladders.parameters():\n",
    "            out += torch.norm( param, 2 )\n",
    "        for param in self.recons.parameters():\n",
    "            out += torch.norm( param, 2 )\n",
    "        out += torch.norm( self.out_conv.weight, 2 )\n",
    "        return out\n",
    "        \n",
    "    def forward( self, x ):\n",
    "        res_out = []\n",
    "        for it in range( x.shape[1] ):\n",
    "            res_out.append( self.getResnetOutputs( x[:,it,:,:,:] ) )\n",
    "        res_0 = torch.stack( [r[0] for r in res_out], dim=1 )\n",
    "        res_1 = torch.stack( [r[1] for r in res_out], dim=1 )\n",
    "        res_2 = torch.stack( [r[2] for r in res_out], dim=1 )\n",
    "        res_3 = torch.stack( [r[3] for r in res_out], dim=1 )\n",
    "        res_4 = torch.stack( [r[4] for r in res_out], dim=1 )\n",
    "        out = self.getReconstructionOutput( res_3, res_4, 4 )\n",
    "        out = self.getReconstructionOutput( res_2, out, 3 )\n",
    "        out = self.getReconstructionOutput( res_1, out, 2 )\n",
    "        out = self.getReconstructionOutput( res_0, out, 1 )\n",
    "        out_l = []\n",
    "        for it in range( x.shape[1] ):\n",
    "            out_f = self.out_conv(out[:,it,:,:,:])\n",
    "            out_f = self.out_shuffle( out_f )\n",
    "            out_f = self.out_act( out_f )\n",
    "            out_l.append( out_f )\n",
    "            \n",
    "        return torch.stack( out_l, dim=1 )\n",
    "        \n",
    "\n",
    "model = PredictionModel(320,224)\n",
    "#print(model.ladders[\"ladder1\"], model.recons[\"recon1\"])\n",
    "inp = torch.rand( 2,6,3,320,224 ).to( device )\n",
    "model = model.to( device )\n",
    "print( model(inp).shape )\n",
    "print( model.getL2Norm() )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
