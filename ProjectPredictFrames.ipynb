{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Gautam Jain, Jannis Horn \n",
    "\n",
    "%matplotlib notebook\n",
    "import time\n",
    "import os\n",
    "from typing import Tuple\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as topt\n",
    "import wandb\n",
    "import cv2\n",
    "import os\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.manual_seed( 666 )\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convGRU code taken from: https://github.com/SreenivasVRao/ConvGRU-ConvLSTM-PyTorch/blob/master/convgru.py\n",
    "from convgru import ConvGRU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LadderLayer( nn.Module ):\n",
    "    def __init__( self, w, h, in_channels, out_channels, use_loc_dep ):\n",
    "        super( LadderLayer, self ).__init__()\n",
    "        if not use_loc_dep: self.conv = nn.Conv2d( in_channels, out_channels, (1,1) )\n",
    "        else: self.conv = LocationAwareConv2d( True, False, w, h, in_channels, out_channels, (1,1) )\n",
    "        self.conv_gru3 = ConvGRU( in_channels, out_channels, (3,3), 1, batch_first=True )\n",
    "        self.conv_gru5 = ConvGRU( in_channels, out_channels, (5,5), 1, batch_first=True )\n",
    "        self.conv_gru7 = ConvGRU( in_channels, out_channels, (7,7), 1, batch_first=True )\n",
    "        \n",
    "    def forward( self, x ):\n",
    "        out_l = []\n",
    "        for it in range( x.shape[1] ):\n",
    "            out_l.append( self.conv( x[:,it,:,:,:] ) )\n",
    "        out = torch.stack( out_l, dim=1 )\n",
    "        out3,_ = self.conv_gru3( x )\n",
    "        out5,_ = self.conv_gru5( x )\n",
    "        out7,_ = self.conv_gru7( x )\n",
    "        return torch.cat( [out, out3[0], out5[0], out7[0]], dim=2 )\n",
    "        \n",
    "        \n",
    "class ReconstructionLayer( nn.Module ):\n",
    "    def __init__( self, in_channels, use_btnorm ):\n",
    "        super( ReconstructionLayer, self ).__init__()\n",
    "        if not use_btnorm: self.relu = nn.ReLU()\n",
    "        else: self.relu = nn.Sequential( nn.ReLU(), nn.BatchNorm2d( in_channels ) )\n",
    "        self.conv1 = nn.Conv2d( in_channels, 1024, (3,3), padding=1 )\n",
    "        self.shuffle = nn.PixelShuffle( 2 )\n",
    "        self.conv2 = nn.Conv2d( 256, 64, (3,3), padding=1 )\n",
    "        \n",
    "    def forward( self, x ):\n",
    "        out = self.relu( x )\n",
    "        out = self.conv1( out )\n",
    "        out = self.shuffle( out )\n",
    "        out = self.conv2( out )\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class LocationAwareConv2d(torch.nn.Conv2d):\n",
    "    def __init__(self,locationAware,gradient,w,h,in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        if locationAware:\n",
    "            self.locationBias=torch.nn.Parameter(torch.zeros(w,h,3))\n",
    "            self.locationEncode=torch.autograd.Variable(torch.ones(w,h,3))\n",
    "            if gradient:\n",
    "                for i in range(w):\n",
    "                    self.locationEncode[i,:,1]=self.locationEncode[:,i,0]=i/float(w-1)\n",
    "        \n",
    "        self.up=torch.nn.Upsample(size=(w,h), mode='bilinear', align_corners=False)\n",
    "        self.w=w\n",
    "        self.h=h\n",
    "        self.locationAware=locationAware\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        if self.locationAware:\n",
    "            if self.locationBias.device != inputs.device:\n",
    "                self.locationBias=self.locationBias.to(inputs.get_device())\n",
    "            if self.locationEncode.device != inputs.device:\n",
    "                self.locationEncode=self.locationEncode.to(inputs.get_device())\n",
    "            b=self.locationBias*self.locationEncode\n",
    "        convRes=super().forward(inputs)\n",
    "        if convRes.shape[2]!=self.w and convRes.shape[3]!=self.h:\n",
    "            convRes=self.up(convRes)\n",
    "        if self.locationAware:\n",
    "            return convRes+b[:,:,0]+b[:,:,1]+b[:,:,2]\n",
    "        else:\n",
    "            return convRes\n",
    "        \n",
    "    def  __str__( self ):\n",
    "        return( \"LocationAware{}, LocAware={}, gradient={}\".format( super().__str__(), self.locationAware, self.Gradient ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3, 160, 112])\n",
      "PredictionModel(\n",
      "  (resnet18): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      "  (ladders): ModuleDict(\n",
      "    (ladder1): LadderLayer(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (conv_gru3): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (reset_gate): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (out_gate): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_gru5): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "            (reset_gate): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "            (out_gate): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_gru7): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "            (reset_gate): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "            (out_gate): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ladder2): LadderLayer(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (conv_gru3): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (reset_gate): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (out_gate): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_gru5): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "            (reset_gate): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "            (out_gate): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_gru7): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "            (reset_gate): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "            (out_gate): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ladder3): LadderLayer(\n",
      "      (conv): LocationAwareConv2d(\n",
      "        128, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (up): Upsample(size=(40, 28), mode=bilinear)\n",
      "      )\n",
      "      (conv_gru3): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (reset_gate): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (out_gate): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_gru5): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(192, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "            (reset_gate): Conv2d(192, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "            (out_gate): Conv2d(192, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_gru7): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(192, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "            (reset_gate): Conv2d(192, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "            (out_gate): Conv2d(192, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ladder4): LadderLayer(\n",
      "      (conv): LocationAwareConv2d(\n",
      "        256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (up): Upsample(size=(20, 14), mode=bilinear)\n",
      "      )\n",
      "      (conv_gru3): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (reset_gate): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (out_gate): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_gru5): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(320, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "            (reset_gate): Conv2d(320, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "            (out_gate): Conv2d(320, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_gru7): ConvGRU(\n",
      "        (cell_list): ModuleList(\n",
      "          (0): ConvGRUCell(\n",
      "            (update_gate): Conv2d(320, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "            (reset_gate): Conv2d(320, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "            (out_gate): Conv2d(320, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (recons): ModuleDict(\n",
      "    (recon1): ReconstructionLayer(\n",
      "      (relu): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv1): Conv2d(320, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (shuffle): PixelShuffle(upscale_factor=2)\n",
      "      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (recon2): ReconstructionLayer(\n",
      "      (relu): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv1): Conv2d(320, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (shuffle): PixelShuffle(upscale_factor=2)\n",
      "      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (recon3): ReconstructionLayer(\n",
      "      (relu): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv1): Conv2d(320, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (shuffle): PixelShuffle(upscale_factor=2)\n",
      "      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (recon4): ReconstructionLayer(\n",
      "      (relu): ReLU()\n",
      "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (shuffle): PixelShuffle(upscale_factor=2)\n",
      "      (conv2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (out_conv): Conv2d(320, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PredictionModel( nn.Module ):\n",
    "    \n",
    "    def __init__( self, w, h ):\n",
    "        super( PredictionModel, self ).__init__()\n",
    "        self.resnet18 = models.resnet18( pretrained=True )\n",
    "        self.ladders = nn.ModuleDict( {\n",
    "            \"ladder1\": LadderLayer( int(w/2), int(h/2), 64, 64, False ),\n",
    "            \"ladder2\": LadderLayer( int(w/4), int(h/4), 64, 64, False ),\n",
    "            \"ladder3\": LadderLayer( int(w/8), int(h/8), 128, 64, True ),\n",
    "            \"ladder4\": LadderLayer( int(w/16), int(h/16), 256, 64, True ) \n",
    "        } )\n",
    "        self.recons = nn.ModuleDict( {\n",
    "            \"recon1\": ReconstructionLayer( 64*5, True ),\n",
    "            \"recon2\": ReconstructionLayer( 64*5, True ),\n",
    "            \"recon3\": ReconstructionLayer( 64*5, True ),\n",
    "            \"recon4\": ReconstructionLayer( 512, False )\n",
    "        } )\n",
    "        self.out_conv = nn.Conv2d( 320, 12, (1,1) )\n",
    "        self.out_shuffle = nn.PixelShuffle( 2 )\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def getResnetOutputs( self, x ):\n",
    "        inp_c = self.resnet18.conv1( x )\n",
    "        inp = self.resnet18.bn1( inp_c )\n",
    "        inp = self.resnet18.relu( inp )\n",
    "        inp = self.resnet18.maxpool( inp )\n",
    "        res_out1 = self.resnet18.layer1( inp )\n",
    "        res_out2 = self.resnet18.layer2( res_out1 )\n",
    "        res_out3 = self.resnet18.layer3( res_out2 )\n",
    "        res_out4 = self.resnet18.layer4( res_out3 )\n",
    "        return ( inp_c, res_out1, res_out2, res_out3, res_out4 )\n",
    "    \n",
    "    def getReconstructionOutput( self, res_x, rec_x, l_it ):\n",
    "        l_out = self.ladders[\"ladder{}\".format(l_it)]( res_x )\n",
    "        r_out_l = []\n",
    "        for it in range( rec_x.shape[1] ):\n",
    "            r_out_l.append( self.recons[\"recon{}\".format(l_it)]( rec_x[:,it,:,:,:] ) )\n",
    "        r_out = torch.stack( r_out_l, dim=1 )\n",
    "        out = torch.cat( [r_out, l_out], dim=2 )\n",
    "        return out\n",
    "        \n",
    "    def forward( self, x ):\n",
    "        res_out = []\n",
    "        for it in range( x.shape[1] ):\n",
    "            res_out.append( self.getResnetOutputs( x[:,it,:,:,:] ) )\n",
    "        res_0 = torch.stack( [r[0] for r in res_out], dim=1 )\n",
    "        res_1 = torch.stack( [r[1] for r in res_out], dim=1 )\n",
    "        res_2 = torch.stack( [r[2] for r in res_out], dim=1 )\n",
    "        res_3 = torch.stack( [r[3] for r in res_out], dim=1 )\n",
    "        res_4 = torch.stack( [r[4] for r in res_out], dim=1 )\n",
    "        out = self.getReconstructionOutput( res_3, res_4, 4 )\n",
    "        out = self.getReconstructionOutput( res_2, out, 3 )\n",
    "        out = self.getReconstructionOutput( res_1, out, 2 )\n",
    "        out = self.getReconstructionOutput( res_0, out, 1 )\n",
    "        out_l = []\n",
    "        for it in range( x.shape[1] ):\n",
    "            out_f = self.out_conv(out[:,it,:,:,:])\n",
    "            out_f = self.out_shuffle( out_f )\n",
    "            out_f = self.out_act( out_f )\n",
    "            out_l.append( out_f )\n",
    "            \n",
    "        return torch.stack( out_l, dim=1 )\n",
    "        \n",
    "\n",
    "model = PredictionModel(320,224)\n",
    "#print(model.ladders[\"ladder1\"], model.recons[\"recon1\"])\n",
    "inp = torch.rand( 2,6,3,320,224 ).to( device )\n",
    "model = model.to( device )\n",
    "print( model(inp).shape )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
